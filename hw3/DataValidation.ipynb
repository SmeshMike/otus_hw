{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d632b92-6c7d-4eef-80ee-56f349837969",
   "metadata": {},
   "source": [
    "# Подготовка для работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d1b2c3-eedf-4e9d-b511-942111d143ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.3\"\n",
    "\n",
    "import pydeequ\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99b9e73-5ed6-4eed-8881-dbf4936b5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1e24638b-4b7e-4ad0-a348-e4f96b187ddb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.7-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 812ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.7-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   2   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1e24638b-4b7e-4ad0-a348-e4f96b187ddb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/22ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/com.amazon.deequ_deequ-2.0.7-spark-3.3.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.scalanlp_breeze_2.12-0.13.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.scalanlp_breeze-macros_2.12-0.13.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.apache.commons_commons-math3-3.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.spire-math_spire_2.12-0.13.0.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/com.chuusai_shapeless_2.12-2.3.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/junit_junit-4.8.2.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.spire-math_spire-macros_2.12-0.13.0.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.typelevel_machinist_2.12-0.6.1.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:29 WARN Client: Same path resource file:///home/ubuntu/.ivy2/jars/org.typelevel_macro-compat_2.12-1.1.1.jar added multiple times to distributed cache.\n",
      "24/06/15 11:07:34 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to dataproc/hadoop/var/log/spark/apps/application_1718438779051_0012.inprogress. This is unsupported\n",
      "24/06/15 11:07:34 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.executor.instances\", 7) \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.driver.cores\", \"1\") \\\n",
    ".config(\"spark.executor.cores\", \"4\") \\\n",
    ".config(\"spark.executor.memory\", \"10g\") \\\n",
    ".config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\\n",
    ".config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    ".getOrCreate()\n",
    "\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", \"https://storage.yandexcloud.net\")\n",
    "hadoopConf.set(\"fs.s3a.access.key\", \"***\")\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", \"***\")\n",
    "hadoopConf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec5b139-dbf3-41e6-9627-f7aea3572ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema():\n",
    "      schema = StructType() \\\n",
    "            .add(\"tranaction_id\",IntegerType()) \\\n",
    "            .add(\"tx_datetime\",StringType()) \\\n",
    "            .add(\"customer_id\",IntegerType()) \\\n",
    "            .add(\"terminal_id\",IntegerType()) \\\n",
    "            .add(\"tx_amount\",DoubleType()) \\\n",
    "            .add(\"tx_time_seconds\",IntegerType()) \\\n",
    "            .add(\"tx_time_days\",IntegerType()) \\\n",
    "            .add(\"tx_fraud\",IntegerType()) \\\n",
    "            .add(\"tx_fraud_scenario\",IntegerType())\n",
    "      return schema\n",
    "\n",
    "def drop_nulls(df):\n",
    "    clear_df = df.filter(df.tranaction_id.isNotNull()) \\\n",
    "    .filter(df.tx_datetime.isNotNull()) \\\n",
    "    .filter(df.customer_id.isNotNull()) \\\n",
    "    .filter(df.terminal_id.isNotNull()) \\\n",
    "    .filter(df.tx_amount.isNotNull()) \\\n",
    "    .filter(df.tx_time_seconds.isNotNull()) \\\n",
    "    .filter(df.tx_time_days.isNotNull()) \\\n",
    "    .filter(df.tx_fraud.isNotNull()) \\\n",
    "    .filter(df.tx_fraud_scenario.isNotNull())\n",
    "    return clear_df\n",
    "\n",
    "def drop_dublicates_by_tranaction_id(df):\n",
    "    return df.dropDuplicates(['tranaction_id'])\n",
    "\n",
    "def filter_zero_value_tranzation(df):\n",
    "    return df.filter(df.tx_amount != 0.0)\n",
    "\n",
    "def filter_negative_customer_id(df):\n",
    "    return df.filter(df.customer_id > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8e573-add8-4aef-906c-247eef946bc4",
   "metadata": {},
   "source": [
    "# Реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af4ecb1-c2a0-4a24-9925-cffa8ef53b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DF row count 1879751273\n"
     ]
    }
   ],
   "source": [
    "### Читаем файл по предварительно подготовленной схеме\n",
    "\n",
    "schema = get_schema()\n",
    "original_df = spark.read.options(delimiter=\",\").schema(schema).csv(\"s3a://mykochecnyuk-bucket/fraud-data/*.txt\")\n",
    "print(\"Original DF row count \", original_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236451bc-359d-43c5-a101-0a4230196472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF No nulls row count  1879753826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df No zero Transatctions row count  1879718566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(906 + 1) / 907]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF No negtive customet IDS row count  1879701255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Фильтруем грязные данные \n",
    "df = drop_dublicates_by_tranaction_id(original_df)\n",
    "df = drop_nulls(df)\n",
    "print(\"DF No nulls row count \",df.count())\n",
    "df = filter_zero_value_tranzation(df)\n",
    "print(\"Df No zero Transatctions row count \",df.count())\n",
    "df = filter_negative_customer_id(df)\n",
    "print(\"DF No negtive customet IDS row count \",df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c8053c-cdb6-4c4d-915e-26727c6a4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Делим DF на 200 частей и сохраняем\n",
    "\n",
    "df.coalesce(200).write.parquet(\"s3a://mykochecnyuk-bucket/cleaned_data.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417040c-8c03-4a6b-8304-7a1c523d6df4",
   "metadata": {},
   "source": [
    "# Опровергнутые гипотезы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19cf38a-f389-4700-b654-646ee340383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Расхождение формата строк с датами\n",
    "\n",
    "df.count() - df.filter(df.tx_datetime.rlike(\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38df6a9-91c8-416a-a0c7-b2dddb85c0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Расхождение мошеннического сценария и факта мошенничества\n",
    "\n",
    "df.select([\"tx_fraud_scenario\", \"tx_fraud\"]).filter(df.tx_fraud_scenario > 0).filter(df.tx_fraud == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6611c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
